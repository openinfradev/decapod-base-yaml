apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: prometheus-operator
  name: prometheus-operator
spec:
  helmVersion: v3
  chart:
    repository: https://prometheus-community.github.io/helm-charts
    name: prometheus-operator
    version: 9.3.2
  releaseName: prometheus-operator
  targetNamespace: lma
  values:
    alertmanager:
      enabled: false
    coreDns:
      enabled: false
    defaultRules:
      create: false
    grafana:
      enabled: false
    kubeApiServer:
      enabled: false
    kubeControllerManager:
      enabled: false
    kubeDns:
      enabled: false
    kubeProxy:
      enabled: false
    kubeScheduler:
      enabled: false
    kubeStateMetrics:
      enabled: false
    kubelet:
      enabled: false
    nodeExporter:
      enabled: false
    prometheusOperator:
      enabled: true
      nodeSelector: {}  # TO_BE_FIXED
      createCustomResource: true
      cleanupCustomResource: true
      cleanupCustomResourceBeforeInstall: true
      hyperkubeImage:
        tag: v1.17.6
    prometheus:
      enabled: false
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: eck-operator
  name: eck-operator
spec:
  helmVersion: v3
  chart:
    repository: https://helm.elastic.co
    name: eck-operator
    version: 1.3.1
  releaseName: eck-operator
  targetNamespace: elastic-system
  values:
    installCRDs: true
    replicaCount: 1
    config:
      containerRegistry: docker.elastic.co
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: prometheus
  name: prometheus
spec:
  helmVersion: v3
  chart:
    repository: https://prometheus-community.github.io/helm-charts
    name: prometheus-operator
    version: 9.3.2
  releaseName: prometheus
  targetNamespace: lma
  values:
    alertmanager:
      enabled: false
    coreDns:
      enabled: true
      serviceMonitor:
        interval: TO_BE_FIXED
    defaultRules:
      create: false
    grafana:
      enabled: false
    kubeApiServer:
      enabled: true
      serviceMonitor:
        interval: TO_BE_FIXED
    kubeControllerManager:
      enabled: true
      serviceMonitor:
        interval: TO_BE_FIXED
    kubeDns:
      enabled: false
      serviceMonitor:
        interval: TO_BE_FIXED
    kubeEtcd:
      enabled: true
      serviceMonitor:
        interval: TO_BE_FIXED
      endpoints: [] # TO_BE_FIXED
      serviceMonitor:
        caFile: /etc/prometheus/secrets/etcd-client-cert/etcd-ca
        certFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client
        insecureSkipVerify: false
        keyFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client-key
        scheme: https
        serverName: localhost
    kubeProxy:
      enabled: true
      serviceMonitor:
        interval: TO_BE_FIXED
    kubeScheduler:
      enabled: true
      serviceMonitor:
        interval: TO_BE_FIXED
    kubeStateMetrics:
      enabled: false
    kubelet:
      enabled: false
    nodeExporter:
      enabled: false
    fullnameOverride: lma
    prometheus:
      prometheusSpec:
        retention: TO_BE_FIXED
        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: TO_BE_FIXED
              accessModes: ["ReadWriteOnce"]
              resources:
                requests:
                  storage: TO_BE_FIXED
        externalLabels:
          taco_cluster: TO_BE_FIXED
        nodeSelector:
          taco-lma: TO_BE_FIXED
        replicas: 1
        ruleNamespaceSelector:
          matchLabels:
            name: lma
        ruleSelectorNilUsesHelmValues: false
        secrets:
        - etcd-client-cert
        serviceMonitorNamespaceSelector:
          matchLabels:
            name: lma
        serviceMonitorSelectorNilUsesHelmValues: false
      service:
        nodePort: 30008
        type: NodePort
    prometheusOperator:
      admissionWebhooks:
        enabled: false
      cleanupCustomResource: false
      cleanupCustomResourceBeforeInstall: false
      createCustomResource: false
      enabled: false
      hyperkubeImage:
        tag: v1.17.6
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: prometheus-fed-master
  name: prometheus-fed-master
spec:
  helmVersion: v3
  chart:
    repository: https://prometheus-community.github.io/helm-charts
    name: prometheus-operator
    version: 9.3.2
  releaseName: prometheus-fed-master
  targetNamespace: fed
  values:
    alertmanager:
      alertmanagerSpec:
        nodeSelector: {} # TO_BE_FIXED
        retention: TO_BE_FIXED
      config:
        global:
          slack_api_url: TO_BE_FIXED
          smtp_auth_password: null
          smtp_auth_username: null
          smtp_from: null
          smtp_smarthost: null
        receivers:
        - name: default-alert
          slack_configs:
          - channel: '#doj-noti'
            send_resolved: true
            text: |-
              {{ if or (and (eq (len .Alerts.Firing) 1) (eq (len .Alerts.Resolved) 0)) (and (eq (len .Alerts.Firing) 0) (eq (len .Alerts.Resolved) 1)) }}
                {{ range .Alerts.Firing }}{{ .Annotations.message }}{{ end }}{{ range .Alerts.Resolved }}{{ .Annotations.message }}{{ end }}
              {{ else }}
              {{ if gt (len .Alerts.Firing) 0 }}
              *Alerts Firing:*
                {{ range .Alerts.Firing }}- {{ .Labels.alertname  }}: {{ .Annotations.message }}
              {{ end }}{{ end }}
              {{ if gt (len .Alerts.Resolved) 0 }}
              *Alerts Resolved:*
                {{ range .Alerts.Resolved }}- {{ .Labels.alertname }}: {{ .Annotations.message }}
              {{ end }}{{ end }}
              {{ end }}
            title: '[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ if or (and (eq (len .Alerts.Firing) 1) (eq (len.Alerts.Resolved) 0)) (and (eq (len .Alerts.Firing) 0) (eq (len .Alerts.Resolved) 1)) }} {{ range .Alerts.Firing }}{{ .Labels.alertname }}{{ end }}{{ range .Alerts.Resolved }}{{ .Labels.alertname }}{{ end }}{{ end }}'
            username: Prometheus
        - name: slack-alert
          slack_configs:
          - channel: '#doj-critical-alert' #FIXME
            send_resolved: true
            text: |-
              {{ if or (and (eq (len .Alerts.Firing) 1) (eq (len .Alerts.Resolved) 0)) (and (eq (len .Alerts.Firing) 0) (eq (len .Alerts.Resolved) 1)) }}
                {{ range .Alerts.Firing }}{{ .Annotations.message }}{{ end }}{{ range .Alerts.Resolved }}{{ .Annotations.message }}{{ end }}
              {{ else }}
              {{ if gt (len .Alerts.Firing) 0 }}
              *Alerts Firing:*
                {{ range .Alerts.Firing }}- {{ .Labels.alertname }}: {{ .Annotations.message }}
              {{ end }}{{ end }}
              {{ if gt (len .Alerts.Resolved) 0 }}
              *Alerts Resolved:*
                {{ range .Alerts.Resolved }}- {{ .Labels.alertname }}: {{ .Annotations.message }}
              {{ end }}{{ end }}
              {{ end }}
            title: '[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ if or (and (eq (len .Alerts.Firing) 1) (eq (len.Alerts.Resolved) 0)) (and (eq (len .Alerts.Firing) 0) (eq (len .Alerts.Resolved) 1)) }} {{ range .Alerts.Firing }}{{ .Labels.alertname }}{{ end }}{{ range .Alerts.Resolved }}{{ .Labels.alertname }}{{ end }}{{ end }}'
            username: Prometheus
        - name: telegram-alert
          webhook_configs:
          - send_resolved: true
            url: http://prometheus-bot:9087/alert/-GROUP_ID
        route:
          group_by:
          - alertname
          group_wait: 10s
          receiver: default-alert
          repeat_interval: 1h
          routes:
          - group_by:
            - alertname
            match:
              severity: page
            receiver: slack-alert
      enabled: true
    coreDns:
      enabled: false
    defaultRules:
      create: false
    fullnameOverride: fed-master
    grafana:
      enabled: false
    kubeApiServer:
      enabled: false
    kubeControllerManager:
      enabled: false
    kubeDns:
      enabled: false
    kubeEtcd:
      enabled: false
    kubeProxy:
      enabled: false
    kubeScheduler:
      enabled: false
    kubeStateMetrics:
      enabled: false
    kubelet:
      enabled: false
    nodeExporter:
      enabled: false
    prometheus:
      prometheusSpec:
        externalLabels:
          taco_cluster: federation-master
        nodeSelector: {} # TO_BE_FIXED
        replicas: 1
        ruleNamespaceSelector:
          matchLabels:
            name: fed
        ruleSelectorNilUsesHelmValues: false
        serviceMonitorNamespaceSelector:
          matchLabels:
            name: fed
        serviceMonitorSelectorNilUsesHelmValues: false
        storageSpec:
          volumeClaimTemplate:
            spec:
              accessModes:
              - ReadWriteOnce
              resources:
                requests:
                  storage: TO_BE_FIXED
              storageClassName: TO_BE_FIXED
      service:
        nodePort: 30018
        type: NodePort
    prometheusOperator:
      admissionWebhooks:
        enabled: false
      cleanupCustomResource: false
      cleanupCustomResourceBeforeInstall: false
      createCustomResource: false
      enabled: false
      hyperkubeImage:
        tag: v1.17.6
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: kube-state-metrics
  name: kube-state-metrics
spec:
  helmVersion: v3
  chart:
    # repository: https://kubernetes.github.io/kube-state-metrics
    # --------------------------------------------------------------------
    # this repo is not working on 2021.1.6
    # several issues are opend for this
    #    - https://github.com/kubernetes/kube-state-metrics/issues/1331
    #    - https://github.com/kubernetes/kube-state-metrics/issues/1337
    #    - https://github.com/kubernetes/kube-state-metrics/issues/1342
    # so, i'll use bitnami before the issues are closed
    repository: https://charts.bitnami.com/bitnami
    name: kube-state-metrics
    # version: 2.9.4
    version: 1.1.3
  releaseName: kube-state-metrics
  targetNamespace: lma
  values:
    nodeSelector: {} # TO_BE_FIXED
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: prometheus-node-exporter
  name: prometheus-node-exporter
spec:
  helmVersion: v3
  chart:
    repository: https://prometheus-community.github.io/helm-charts
    name: prometheus-node-exporter
    version: 1.11.2
  releaseName: prometheus-node-exporter
  targetNamespace: lma
  values:
    extraArgs:
    - --no-collector.hwmon
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: prometheus-pushgateway
  name: prometheus-pushgateway
spec:
  helmVersion: v3
  chart:
    repository: https://prometheus-community.github.io/helm-charts
    name: prometheus-pushgateway
    version: 1.5.1
  releaseName: prometheus-pushgateway
  targetNamespace: lma
  values:
    service:
      nodePort: 30010
      type: NodePort
    nodeSelector: {} # TO_BE_FIXED
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: prometheus-process-exporter
  name: prometheus-process-exporter
spec:
  helmVersion: v3
  chart:
    repository: https://openinfradev.github.io/hanu-helm-repo
    name: prometheus-process-exporter
    version: 0.1.0
    skipDepUpdate: true
  releaseName: prometheus-process-exporter
  targetNamespace: lma
  values:
    images:
      pull_policy: Always
    labels:
      process_exporter:
        process_selector_key: process-exporter
        process_selector_value: enabled
    pod:
      lifecycle:
        upgrades:
          daemonsets:
            process_exporter:
              max_unavailable: 30%
      mandatory_access_control:
        type: null
      tolerations:
        process_exporter:
          enabled: true
          tolerations:
          - key: node-role.kubernetes.io/master
            operator: Exists
          - key: node-role.kubernetes.io/node
            operator: Exists
    conf:
      processes: dockerd,kubelet,kube-proxy,ntpd
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: eck-resource
  name: eck-resource
spec:
  helmVersion: v3
  chart:
    repository: https://openinfradev.github.io/hanu-helm-repo
    name: eck-resource
    version: 1.0.0
  releaseName: eck-resource
  targetNamespace: lma
  values:
    elasticsearch:
      version: 7.5.1
      adminPassword: TO_BE_FIXED
      enabled: true 
      count: 3        # FIXME
      nodeSets:
        master:
          enabled: true
          nodeSelector: {} # TO_BE_FIXED
          count: 3
          javaOpts: TO_BE_FIXED
          limitCpu: TO_BE_FIXED
          limitMem: TO_BE_FIXED
          pvc:
            storageClassName: TO_BE_FIXED
            size: TO_BE_FIXED
        hotdata:
          enabled: true
          nodeSelector: {} # TO_BE_FIXED
          count: 3
          javaOpts: TO_BE_FIXED
          limitCpu: TO_BE_FIXED
          limitMem: TO_BE_FIXED
          pvc:
            storageClassName: TO_BE_FIXED
            size: TO_BE_FIXED
        warmdata:
          enabled: false # TO_BE_FIXED
          nodeSelector: {} # TO_BE_FIXED
          count: 2
          javaOpts: TO_BE_FIXED
          limitCpu: TO_BE_FIXED
          limitMem: TO_BE_FIXED
          pvc:
            storageClassName: TO_BE_FIXED
            size: TO_BE_FIXED
        client:
          enabled: true # TO_BE_FIXED
          nodeSelector: {} # TO_BE_FIXED
          count: TO_BE_FIXED
          javaOpts: TO_BE_FIXED
          limitCpu: TO_BE_FIXED
          limitMem: TO_BE_FIXED
          pvc:
            storageClassName: TO_BE_FIXED
            size: 0.5Gi
    kibana:
      version: 7.5.1
      enabled: true
      http:
        tls:
          selfSignedCertificate:
            disabled: true
        service:
          spec:
            type: NodePort
            ports:
            - name: http
              nodePort: 30001
              targetPort: 5601
              port: 5601
      limitCpu: TO_BE_FIXED
      limitMem: TO_BE_FIXED
      nodeSelector: {} # TO_BE_FIXED
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: grafana
  name: grafana
spec:
  helmVersion: v3
  chart:
    repository: https://grafana.github.io/helm-charts
    name: grafana
    version: 5.5.7
  releaseName: grafana
  targetNamespace: fed
  values:
    adminPassword: password
    sidecar:
      dashboards:
        enabled: true
        label: grafana_dashboard
      datasources:
        enabled: true
        label: grafana_datasource
    service:
      type: NodePort
      nodePort: 30009
    persistence:
      enabled: true
      size: 10G #FIXME
      storageClassName: TO_BE_FIXED
    grafana.ini:
      plugins:
        vonage-status-panel: true
        grafana-piechart-panel: true
        devopsprodigy-kubegraf-app: true
    plugins:
    - vonage-status-panel
    - grafana-piechart-panel
    - devopsprodigy-kubegraf-app
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: fluentbit-operator
  name: fluentbit-operator
spec:
  helmVersion: v3
  chart:
    repository: https://openinfradev.github.io/hanu-helm-repo
    name: fluentbit-operator
    version: 1.0.9
    skipDepUpdate: true
  releaseName: fluentbit-operator
  targetNamespace: lma
  values:
    global:
      base_cluster_url: TO_BE_FIXED   #FIXME
    fluentbitOperator:
      enabled: true
      createCustomResource: true
      cleanupCustomResource: true
      nodeSelector: {} # TO_BE_FIXED
      resources:
        limits:
          cpu: "2"
          memory: 200Mi
        requests:
          cpu: 100m
          memory: 20Mi
    fluentbit:
      enabled: false
    logExporter:
      enabled: true
      nodeSelector: {} # TO_BE_FIXED
      serviceMonitor:
        enabled: false
        interval: 1m
    image:
      hyperkube:
        tag: v1.17.6
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: fluentbit
  name: fluentbit
spec:
  helmVersion: v3
  chart:
    repository: https://openinfradev.github.io/hanu-helm-repo
    name: fluentbit-operator
    version: 1.0.9
    skipDepUpdate: true
  releaseName: fluentbit
  targetNamespace: lma
  values:
    global:
      base_cluster_url: TO_BE_FIXED   #FIXME
      nodeSelector: {} # TO_BE_FIXED
    fluentbitOperator:
      enabled: false
      createCustomResource: false
      cleanupCustomResource: false
    fluentbit:
      enabled: true
      clusterName: TO_BE_FIXED
      daemonset:
        spec:
          pod:
            tolerations:
            - key: node-role.kubernetes.io/master
              operator: Exists
            - key: node-role.kubernetes.io/node
              operator: Exists
      targetLogs:
      - name: dockerlog
        tag: kube.*
        path: /var/log/containers/*.log
        type: fluent
        index: container
        parser: docker
        memBufLimit: 20MB
        bufferChunkSize: 2M
        bufferMaxSize: 5M
        multi_index:
        - key: "$kubernetes['namespace_name']"
          value: "kube-system|lma|fed|argo|openstack"
          index: platform
      - name: syslog
        tag: syslog.*
        path: /var/log/messages
        type: syslog
        index: syslog
        parser: syslog-rfc5424
      outputs:
        es:
          enabled: true
          username: ZWxhc3RpYw==  #FIXME (elastic)
          password: dGFjb3dvcmQ=  #FIXME (tacoword)
          host: TO_BE_FIXED
          port: 9200
        http:
          enabled: true
      esTemplate:
        enabled: true
        url: TO_BE_FIXED
        username: elastic
        password: tacoword
        ilms:
        - name: hot-delete-14days
          json:
            policy:
              phases:
                hot:
                  actions:
                    rollover:
                      max_size: 30gb
                      max_age: 1d
                      max_docs: 5000000000
                    set_priority:
                      priority: 100
                delete:
                  min_age: 14d
                  actions:
                    delete: {}
        - name: hot-delete-7days
          json:
            policy:
              phases:
                hot:
                  actions:
                    rollover:
                      max_size: 30gb
                      max_age: 1d
                      max_docs: 5000000000
                    set_priority:
                      priority: 100
                delete:
                  min_age: 7d
                  actions:
                    delete: {}
        - name: hot-delete-3hour
          json:
            policy:
              phases:
                hot:
                  actions:
                    rollover:
                      max_size: 30gb
                      max_age: 1h
                      max_docs: 5000000000
                    set_priority:
                      priority: 100
                delete:
                  min_age: 3h
                  actions:
                    delete: {}
        templates:
        - name: platform
          json:
            index_patterns: "platform*"
            settings:
              refresh_interval: 30s
              number_of_shards: 3
              number_of_replicas: 1
              index.lifecycle.name: hot-delete-14days
              index.lifecycle.rollover_alias: platform
            # mappings:
            #   properties:
            #     "@timestamp": 
            #       type: date
            #     cluster: 
            #       type: text
            #     kubernetes:
            #       properties:
            #         container_name:
            #           type: keyword
            #           index: true
            #         container_name:
            #           type: keyword
            #           index: true
            #         host: 
            #           type: keyword
            #           index: false
            #         namespace:
            #           type: keyword
            #           index: false
            #         pod_name:
            #           type: keyword
            #           index: true
            #     log: 
            #       type: text
            #     status: 
            #       type: long
            #     time:
            #       type: date
        - name: application
          json:
            index_patterns: "container*"
            settings:
              refresh_interval: 30s
              number_of_shards: 3
              number_of_replicas: 1
              index.lifecycle.name: hot-delete-3hour
              index.lifecycle.rollover_alias: container
            # mappings:
            #   properties:
            #     "@timestamp": 
            #       type: date
            #     cluster: 
            #       type: text
            #     kubernetes:
            #       properties:
            #         container_name:
            #           type: keyword
            #           index: true
            #         container_name:
            #           type: keyword
            #           index: true
            #         host: 
            #           type: keyword
            #           index: false
            #         namespace:
            #           type: keyword
            #           index: false
            #         pod_name:
            #           type: keyword
            #           index: true
            #     log: 
            #       type: text
            #     status: 
            #       type: long
            #     time:
            #       type: date
        - name: syslog
          json:
            index_patterns: "syslog*"
            settings:
              refresh_interval: 30s
              number_of_shards: 2
              number_of_replicas: 1
              index.lifecycle.name: hot-delete-14days
              index.lifecycle.rollover_alias: syslog
            # mappings:
            #   properties:
            #     "@timestamp": 
            #       type: date
            #     cluster: 
            #       type: text
            #     hostname: 
            #       type: keyword
            #       index: true
            #     log: 
            #       type: text
      exclude:
      - key: "$kubernetes['container_name']"
        value: kibana|elasticsearch|fluent-bit
      alerts:
        makeAlertRule: true
      nodeSelector: {} # TO_BE_FIXED
    fluentbitOperator:
      enabled: false
      createCustomResource: false
      cleanupCustomResource: false
    logExporter:
      enabled: false
    image:
      hyperkube:
        tag: v1.17.6
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: addons
  name: addons
spec:
  helmVersion: v3
  chart:
    repository: https://openinfradev.github.io/hanu-helm-repo
    name: lma-addons
    version: 1.0.7
  releaseName: addons
  targetNamespace: lma
  values:
    grafanaDashboard:
      enabled: false
    metricbeat:
      enabled: false
      loglevel: error
    prometheusRules:
      aggregation:
        enabled: true
      alert:
        enabled: false
    serviceMonitor:
      processExporter:
        enabled: true
        interval: TO_BE_FIXED
        selector:
          matchLabels:
            application: process_exporter
            component: metrics
            release_group: prometheus-process-exporter
      grafana:
        enabled: false
      ceph:
        enabled: false  #FIXME
        interval: TO_BE_FIXED
        mon_hosts: []
      calico:
        enabled: true
        interval: TO_BE_FIXED
      kubelet:
        enabled: true
        interval: TO_BE_FIXED
    tacoWatcher:
      rbac:
        create: false
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: fed-addons
  name: fed-addons
spec:
  helmVersion: v3
  chart:
    repository: https://openinfradev.github.io/hanu-helm-repo
    name: lma-addons
    version: 1.0.7
  releaseName: fed-addons
  targetNamespace: fed
  values:
    metricbeat:
      enabled: true
      elasticsearch:
        host: TO_BE_FIXED
        username: elastic
        password: tacoword
      kibana:
        host: TO_BE_FIXED
      prometheus:
        hosts: [] # TO_BE_FIXED
      addtionalModules: []
    grafanaDashboard:
      enabled: true
      sidecar:
        datasources:
          prometheusAddress: "fed-master-prometheus:9090"
    serviceMonitor:
      calico:
        enabled: false
      processExporter:
        enabled: false
      grafana:
        enabled: true
        interval: TO_BE_FIXED
      ceph:
        enabled: false
      kubeStateMetrics:
        enabled: false
      nodeExporter:
        enabled: false
      federations: [] # TO_BE_FIXED
    tacoWatcher:
      host: TO_BE_FIXED
      port: 32000
      rbac:
        create: true
      joinCluster:
        enabled: false
        body:
          isMain: true
          kibanaUrl: TO_BE_FIXED
          grafanaUrl: TO_BE_FIXED
          k8sUrl: TO_BE_FIXED
          menu:
            enabled: true
    kibanaInit:
      enabled: true
      url: TO_BE_FIXED
    prometheusRules:
      aggregation:
        enabled: false
      alert:
        enabled: true
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: prometheus-adapter
  name: prometheus-adapter
spec:
  helmVersion: v3
  chart:
    repository: https://prometheus-community.github.io/helm-charts
    name: prometheus-adapter
    version: 2.5.1
  releaseName: prometheus-adapter
  targetNamespace: lma
  values:
    prometheus:
      url: http://lma-prometheus
      port: 9090
    rules:
      default: false
    nodeSelector: {} # TO_BE_FIXED
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: kubernetes-event-exporter
  name: kubernetes-event-exporter
spec:
  helmVersion: v3
  chart:
    repository: https://openinfradev.github.io/hanu-helm-repo
    name: kubernetes-event-exporter
    version: 1.0.0
  releaseName: kubernetes-event-exporter
  targetNamespace: lma
  values:
    conf:
      logLevel: error     # possible level: error, debug,
      logFormat: json
      default:
        hosts: [] # TO_BE_FIXED
        index: kube-events
        user: elastic
        password: tacoword
        additionalDefaultReceivers: []
      additionalRoutes: {}
      additionalReceivers: {}
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: taco-watcher
  name: taco-watcher
spec:
  helmVersion: v3 
  chart:
    repository: https://openinfradev.github.io/hanu-helm-repo
    name: taco-watcher
    version: 0.1.0
  releaseName: taco-watcher
  targetNamespace: fed
  values:
    config:
      grafana:
        authkey: admin:password
      initDB: true
      kibana:
        authkey: elastic:tacoword
      password: password
      username: taco
    nodeSelector: {} # TO_BE_FIXED
    resources:
      storageClassName: TO_BE_FIXED
    service:
      nodePort: 32000
      port: 32000
      proxy_from: 32001
      proxy_to: 32009
      targetPort: 32000
      type: NodePort
  wait: true


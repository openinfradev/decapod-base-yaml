---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: prometheus-operator-crds
  name: prometheus-operator-crds
spec:
  chart:
    type: helmrepo
    repository: https://prometheus-community.github.io/helm-charts
    name: kube-prometheus-stack
    version: 21.0.3
  helmVersion: v3
  releaseName: prometheus-operator-crds
  targetNamespace: lma
  values: {}
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: prometheus-operator
  name: prometheus-operator
spec:
  helmVersion: v3
  chart:
    type: helmrepo
    repository: https://prometheus-community.github.io/helm-charts
    name: kube-prometheus-stack
    version: 21.0.3
  releaseName: prometheus-operator
  targetNamespace: lma
  values:
    fullnameOverride: prometheus-operator
    defaultRules:
      create: false
    global:
      rbac:
        create: true
    alertmanager:
      enabled: false
    grafana:
      enabled: false
    kubeApiServer:
      enabled: false
    kubelet:
      enabled: false
    kubeControllerManager:
      enabled: false
    coreDns:
      enabled: false
    kubeDns:
      enabled: false
    kubeEtcd:
      enabled: false
    kubeScheduler:
      enabled: false
    kubeProxy:
      enabled: false
    kubeStateMetrics:
      enabled: false
#   kube-state-metrics: # subchart configuration
    nodeExporter:
      enabled: false
#   prometheus-node-exporter: # subchart configuration
    prometheusOperator:
      enabled: true
      prometheusConfigReloader:
        image:
          registry: quay.io
          repository: prometheus-operator/prometheus-config-reloader
          # if not set appVersion field from Chart.yaml is used
          tag: ""
      thanosImage:
        registry: quay.io
        repository: thanos/thanos
        tag: v0.30.2
      nodeSelector: {}  # TO_BE_FIXED
      createCustomResource: true
      cleanupCustomResource: true
      cleanupCustomResourceBeforeInstall: true
    prometheus:
      enabled: false
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: prometheus
  name: prometheus
spec:
  helmVersion: v3
  chart:
    type: helmrepo
    repository: https://prometheus-community.github.io/helm-charts
    name: kube-prometheus-stack
    version: 21.0.3
  releaseName: prometheus
  targetNamespace: lma
  values:
    defaultRules:
      create: false
    global:
      rbac:
        create: true
    alertmanager:
      alertmanagerSpec:
        nodeSelector: {} # TO_BE_FIXED
        retention: TO_BE_FIXED
      config:
        global:
          slack_api_url: TO_BE_FIXED
          smtp_auth_password: null
          smtp_auth_username: null
          smtp_from: null
          smtp_smarthost: null
        receivers:
        - name: default-alert
          slack_configs:
          - channel: '#doj-noti'
            send_resolved: true
            text: |-
              {{ if or (and (eq (len .Alerts.Firing) 1) (eq (len .Alerts.Resolved) 0)) (and (eq (len .Alerts.Firing) 0) (eq (len .Alerts.Resolved) 1)) }}
                {{ range .Alerts.Firing }}{{ .Annotations.message }}{{ end }}{{ range .Alerts.Resolved }}{{ .Annotations.message }}{{ end }}
              {{ else }}
              {{ if gt (len .Alerts.Firing) 0 }}
              *Alerts Firing:*
                {{ range .Alerts.Firing }}- {{ .Labels.alertname  }}: {{ .Annotations.message }}
              {{ end }}{{ end }}
              {{ if gt (len .Alerts.Resolved) 0 }}
              *Alerts Resolved:*
                {{ range .Alerts.Resolved }}- {{ .Labels.alertname }}: {{ .Annotations.message }}
              {{ end }}{{ end }}
              {{ end }}
            title: '[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ if or (and (eq (len .Alerts.Firing) 1) (eq (len.Alerts.Resolved) 0)) (and (eq (len .Alerts.Firing) 0) (eq (len .Alerts.Resolved) 1)) }} {{ range .Alerts.Firing }}{{ .Labels.alertname }}{{ end }}{{ range .Alerts.Resolved }}{{ .Labels.alertname }}{{ end }}{{ end }}'
            username: Prometheus
        - name: slack-alert
          slack_configs:
          - channel: '#doj-critical-alert' #FIXME
            send_resolved: true
            text: |-
              {{ if or (and (eq (len .Alerts.Firing) 1) (eq (len .Alerts.Resolved) 0)) (and (eq (len .Alerts.Firing) 0) (eq (len .Alerts.Resolved) 1)) }}
                {{ range .Alerts.Firing }}{{ .Annotations.message }}{{ end }}{{ range .Alerts.Resolved }}{{ .Annotations.message }}{{ end }}
              {{ else }}
              {{ if gt (len .Alerts.Firing) 0 }}
              *Alerts Firing:*
                {{ range .Alerts.Firing }}- {{ .Labels.alertname }}: {{ .Annotations.message }}
              {{ end }}{{ end }}
              {{ if gt (len .Alerts.Resolved) 0 }}
              *Alerts Resolved:*
                {{ range .Alerts.Resolved }}- {{ .Labels.alertname }}: {{ .Annotations.message }}
              {{ end }}{{ end }}
              {{ end }}
            title: '[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ if or (and (eq (len .Alerts.Firing) 1) (eq (len.Alerts.Resolved) 0)) (and (eq (len .Alerts.Firing) 0) (eq (len .Alerts.Resolved) 1)) }} {{ range .Alerts.Firing }}{{ .Labels.alertname }}{{ end }}{{ range .Alerts.Resolved }}{{ .Labels.alertname }}{{ end }}{{ end }}'
            username: Prometheus
        - name: telegram-alert
          webhook_configs:
          - send_resolved: true
            url: http://prometheus-bot:9087/alert/-GROUP_ID
        route:
          group_by:
          - alertname
          group_wait: 10s
          receiver: default-alert
          repeat_interval: 1h
          routes:
          - group_by:
            - alertname
            match:
              severity: page
            receiver: slack-alert
      enabled: true
    grafana:
      enabled: false
    kubeApiServer:
      enabled: true
      serviceMonitor:
        interval: TO_BE_FIXED
    kubelet:
      enabled: false
    kubeControllerManager:
      enabled: false
    coreDns:
      enabled: true
      serviceMonitor:
        interval: TO_BE_FIXED
    kubeDns:
      enabled: false
      serviceMonitor:
        interval: TO_BE_FIXED
    kubeEtcd:
      enabled: true
      endpoints: []
      serviceMonitor:
        interval: TO_BE_FIXED
        caFile: /etc/prometheus/secrets/etcd-client-cert/etcd-ca
        certFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client
        insecureSkipVerify: false
        keyFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client-key
        scheme: https
        serverName: localhost
    kubeScheduler:
      enabled: false
    kubeProxy:
      enabled: true
      serviceMonitor:
        interval: TO_BE_FIXED
    kubeStateMetrics:
      enabled: false
#   kube-state-metrics: # subchart configuration
    nodeExporter:
      enabled: false
#   prometheus-node-exporter: # subchart configuration
    fullnameOverride: lma
    prometheusOperator:
      admissionWebhooks:
        enabled: false
      enabled: false
    prometheus:
      prometheusSpec:
        retention: TO_BE_FIXED
        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: TO_BE_FIXED
              accessModes: ["ReadWriteOnce"]
              resources:
                requests:
                  storage: TO_BE_FIXED
        externalLabels:
          taco_cluster: TO_BE_FIXED
        nodeSelector:
          taco-lma: TO_BE_FIXED
        replicas: 1
        ruleNamespaceSelector:
          matchLabels:
            name: lma
        ruleSelectorNilUsesHelmValues: false
        secrets:
        - etcd-client-cert
        serviceMonitorNamespaceSelector:
          matchLabels:
            name: lma
        serviceMonitorSelectorNilUsesHelmValues: false
        podMonitorNamespaceSelector:
          matchLabels:
            name: lma
        podMonitorSelectorNilUsesHelmValues: false
        thanos:
          version: v0.18.0
          objectStorageConfig:
            key: objstore.yml
            name: TO_BE_FIXED # secret name
          minTime: -3h
      service:
        nodePort: 30008
        type: NodePort
      thanosServiceExternal:
        enabled: true
        annotations: {}
        labels: {}
        portName: grpc
        port: 10901
        targetPort: "grpc"
        type: LoadBalancer
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: eck-operator
  name: eck-operator
spec:
  helmVersion: v3
  chart:
    type: helmrepo
    repository: https://helm.elastic.co
    name: eck-operator
    version: 1.8.0
  releaseName: eck-operator
  targetNamespace: elastic-system
  values:
    installCRDs: true
    replicaCount: 1
    config:
      containerRegistry: docker.elastic.co
    global:
      manifestGen: true
      kubeVersion: 1.22.2
      createOperatorNamespace: true
    softMultiTenancy:
      enabled: false
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: kube-state-metrics
  name: kube-state-metrics
spec:
  helmVersion: v3
  chart:
    type: helmrepo
    # repository: https://kubernetes.github.io/kube-state-metrics
    # --------------------------------------------------------------------
    # this repo is not working on 2021.1.6
    # several issues are opend for this
    #    - https://github.com/kubernetes/kube-state-metrics/issues/1331
    #    - https://github.com/kubernetes/kube-state-metrics/issues/1337
    #    - https://github.com/kubernetes/kube-state-metrics/issues/1342
    # so, i'll use bitnami before the issues are closed
    repository: https://openinfradev.github.io/helm-repo
    name: kube-state-metrics
    # version: 2.9.4
    version: 1.1.3
  releaseName: kube-state-metrics
  targetNamespace: lma
  values:
    nodeSelector: {} # TO_BE_FIXED
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: prometheus-node-exporter
  name: prometheus-node-exporter
spec:
  helmVersion: v3
  chart:
    type: helmrepo
    repository: https://prometheus-community.github.io/helm-charts
    name: prometheus-node-exporter
    version: 1.11.2
  releaseName: prometheus-node-exporter
  targetNamespace: lma
  values:
    # Expose the service to the host network
    hostNetwork: true
    extraArgs:
    - --no-collector.hwmon
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: prometheus-pushgateway
  name: prometheus-pushgateway
spec:
  helmVersion: v3
  chart:
    type: helmrepo
    repository: https://prometheus-community.github.io/helm-charts
    name: prometheus-pushgateway
    version: 1.5.1
  releaseName: prometheus-pushgateway
  targetNamespace: lma
  values:
    service:
      nodePort: 30010
      type: NodePort
    nodeSelector: {} # TO_BE_FIXED
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: prometheus-process-exporter
  name: prometheus-process-exporter
spec:
  helmVersion: v3
  chart:
    type: helmrepo
    repository: https://openinfradev.github.io/helm-repo
    name: prometheus-process-exporter
    version: 0.1.4-skt
    skipDepUpdate: true
  releaseName: prometheus-process-exporter
  targetNamespace: lma
  values:
    images:
      pull_policy: Always
    labels:
      process_exporter:
        process_selector_key: process-exporter
        process_selector_value: enabled
    pod:
      lifecycle:
        upgrades:
          daemonsets:
            process_exporter:
              max_unavailable: 30%
      mandatory_access_control:
        type: null
      tolerations:
        process_exporter:
          enabled: true
          tolerations:
          - key: node-role.kubernetes.io/master
            operator: Exists
          - key: node-role.kubernetes.io/node
            operator: Exists
      # Expose the service to the host network
      hostNetwork: true
    conf:
      processes: dockerd,kubelet,kube-proxy,ntpd
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: eck-resource
  name: eck-resource
spec:
  helmVersion: v3
  chart:
    type: helmrepo
    repository: https://openinfradev.github.io/helm-repo
    name: eck-resource
    version: 1.1.1
  releaseName: eck-resource
  targetNamespace: lma
  values:
    elasticsearch:
      image:
        tag: 7.5.1
      adminPassword: TO_BE_FIXED
      enabled: true
      count: 3        # FIXME
      http:
        service:
          spec:
            type: NodePort
            ports:
            - name: https
              nodePort: 30011
              targetPort: 9200
              port: 9200
      nodeSets:
        master:
          enabled: true
          nodeSelector: {} # TO_BE_FIXED
          count: 3
          javaOpts: TO_BE_FIXED
          limitCpu: TO_BE_FIXED
          limitMem: TO_BE_FIXED
          pvc:
            storageClassName: TO_BE_FIXED
            size: TO_BE_FIXED
        hotdata:
          enabled: true
          nodeSelector: {} # TO_BE_FIXED
          count: 3
          javaOpts: TO_BE_FIXED
          limitCpu: TO_BE_FIXED
          limitMem: TO_BE_FIXED
          pvc:
            storageClassName: TO_BE_FIXED
            size: TO_BE_FIXED
        warmdata:
          enabled: false # TO_BE_FIXED
          nodeSelector: {} # TO_BE_FIXED
          count: 2
          javaOpts: TO_BE_FIXED
          limitCpu: TO_BE_FIXED
          limitMem: TO_BE_FIXED
          pvc:
            storageClassName: TO_BE_FIXED
            size: TO_BE_FIXED
        client:
          enabled: true # TO_BE_FIXED
          nodeSelector: {} # TO_BE_FIXED
          count: TO_BE_FIXED
          javaOpts: TO_BE_FIXED
          limitCpu: TO_BE_FIXED
          limitMem: TO_BE_FIXED
          pvc:
            storageClassName: TO_BE_FIXED
            size: 0.5Gi
    kibana:
      image:
        tag: 7.5.1
      enabled: true
      http:
        tls:
          selfSignedCertificate:
            disabled: true
        service:
          spec:
            type: NodePort
            ports:
            - name: http
              nodePort: 30001
              targetPort: 5601
              port: 5601
      limitCpu: TO_BE_FIXED
      limitMem: TO_BE_FIXED
      nodeSelector: {} # TO_BE_FIXED
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: grafana
  name: grafana
spec:
  helmVersion: v3
  chart:
    type: helmrepo
    repository: https://grafana.github.io/helm-charts
    name: grafana
    version: 6.19.4
  releaseName: grafana
  targetNamespace: lma
  values:
    adminPassword: password
    sidecar:
      dashboards:
        enabled: true
        label: grafana_dashboard
      datasources:
        enabled: true
        label: grafana_datasource
    service:
      type: NodePort
      nodePort: 30009
    persistence:
      enabled: true
      size: 10G #FIXME
      storageClassName: TO_BE_FIXED
    grafana.ini:
      plugins:
        vonage-status-panel: true
        grafana-piechart-panel: true
    plugins:
    - vonage-status-panel
    - grafana-piechart-panel
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: fluent-operator-crds
  name: fluent-operator-crds
spec:
  helmVersion: v3
  chart:
    type: helmrepo
    repository: https://openinfradev.github.io/helm-repo
    name: fluent-operator
    version: 1.7.0
    skipDepUpdate: true
  releaseName: fluent-operator-crds
  targetNamespace: lma
  values: {}
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: fluent-operator
  name: fluent-operator
spec:
  helmVersion: v3
  chart:
    type: helmrepo
    repository: https://openinfradev.github.io/helm-repo
    name: fluent-operator
    version: 1.7.0
    skipDepUpdate: true
  releaseName: fluent-operator
  targetNamespace: lma
  values:
    operator:
      initcontainer:
        repository: "docker"
        tag: "19.03"
      container:
        repository: "kubesphere/fluent-operator"
        tag: "v1.5.0"
      # FluentBit operator resources. Usually user needn't to adjust these.
      resources:
        limits:
          cpu: 1000m
          memory: 200Mi
        requests:
          cpu: 100m
          memory: 20Mi
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: fluentbit
  name: fluentbit
spec:
  helmVersion: v3
  chart:
    type: helmrepo
    repository: https://openinfradev.github.io/helm-repo
    name: fluentbit-resource
    version: 1.2.0
    skipDepUpdate: true
  releaseName: fluentbit
  targetNamespace: lma
  values:
    fullnameOverride: fbcr-taco
    fluentbit:
      enabled: true
      daemonset:
        spec:
          pod:
            tolerations:
            - key: node-role.kubernetes.io/master
              operator: Exists
            - key: node-role.kubernetes.io/node
              operator: Exists
      job:
        spec:
          nodeSelector:
            taco-lma: enabled
      parsers:
      - name: taco-syslog-parser-for-ubuntu # modified from syslog-rfc3164
        regex:
          regex: '^(?<time>[^ ]* {1,2}[^ ]* [^ ]*) (?<host>[^ ]*) (?<ident>[a-zA-Z0-9_\/\.\-]*)(?:\[(?<pid>[0-9]+)\])?(?:[^\:]*\:)? *(?<message>.*)$'
          timeFormat: '%b %d %H:%M:%S'
          timeKeep: false
          timeKey: 'time'
      outputs: { }
      targetLogs: [ ]
      alerts:
        enabled: true
        namespace: taco-system
        message: |-
          {{ $labels.container }} in {{ $labels.pod }} ({{ $labels.taco_cluster }}/{{ $labels.namespace }} ) generate a error due to log = {{ $labels.log }}
        summary: |-
          {{ $labels.container }} in {{ $labels.pod }} ({{ $labels.taco_cluster }}/{{ $labels.namespace }} ) generate a error
        rules: [ ]
      clusterName: TO_BE_FIXED
      exclude:
      - key: $kubernetes['container_name']
        value: kibana|elasticsearch|fluent-bit
    logExporter:
      enabled: true
      serviceMonitor:
        enabled: true
      spec:
        nodeSelector:
          taco-lma: enabled
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: addons
  name: addons
spec:
  helmVersion: v3
  chart:
    type: helmrepo
    repository: https://openinfradev.github.io/helm-repo
    name: lma-addons
    version: 1.8.2
  releaseName: addons
  targetNamespace: lma
  values:
    grafanaDatasource:
      enabled: true
      namespace: lma
      prometheus:
        enabled: true
        url: "thanos-query.lma:9090"
      loki:
        enabled: true
        url: "loki-loki-distributed-gateway.lma"
    grafanaDashboard:
      include:
      - kubernetes
      - node
      - loki
      - calico
      - etcd
      namespace: lma
    serviceMonitor:
      calico:
        enabled: true
        interval: TO_BE_FIXED
      processExporter:
        enabled: true
        interval: TO_BE_FIXED
        selector:
          matchLabels:
            application: process_exporter
            component: metrics
            release_group: prometheus-process-exporter
      istio:
        enabled: true
        interval: TO_BE_FIXED
      jaeger:
        enabled: true
        interval: TO_BE_FIXED
      grafana:
        enabled: true
        interval: TO_BE_FIXED
      argocd:
        enabled: true
        interval: TO_BE_FIXED
      argowf:
        enabled: true
        interval: TO_BE_FIXED
      ceph:
        enabled: false  #FIXME
        interval: TO_BE_FIXED
        mon_hosts: []
      kubeStateMetrics:
        enabled: true
      nodeExporter:
        enabled: true
      pushgateway:
        enabled: true
      kubelet:
        enabled: true
        interval: TO_BE_FIXED
      federations: [] # TO_BE_FIXED
    kibanaInit:
      enabled: false
    prometheusRules:
      aggregation:
        enabled: true
      alert:
        enabled: true
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: prometheus-adapter
  name: prometheus-adapter
spec:
  helmVersion: v3
  chart:
    type: helmrepo
    repository: https://prometheus-community.github.io/helm-charts
    name: prometheus-adapter
    version: 2.5.1
  releaseName: prometheus-adapter
  targetNamespace: lma
  values:
    prometheus:
      url: http://lma-prometheus
      port: 9090
    rules:
      default: false
    nodeSelector: {} # TO_BE_FIXED
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: kubernetes-event-exporter
  name: kubernetes-event-exporter
spec:
  helmVersion: v3
  chart:
    type: helmrepo
    repository: https://openinfradev.github.io/helm-repo
    name: kubernetes-event-exporter
    version: 2.0.0
  releaseName: kubernetes-event-exporter
  targetNamespace: lma
  values:
    clustername: TO_BE_FIXED
    conf:
      logLevel: error     # possible level: error, debug,
      logFormat: json
      recievers: [ ]
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: minio
  name: minio
spec:
  helmVersion: v3
  chart:
    type: helmrepo
    repository: https://openinfradev.github.io/helm-repo
    name: minio
    version: 6.8.3
  releaseName: minio
  targetNamespace: lma
  values:
    accessKey:
      password: TO_BE_FIXED
    secretKey:
      password: TO_BE_FIXED
    defaultBuckets: TO_BE_FIXED
    service:
      type: NodePort
      nodePort: 30002
    persistence:
      storageClass: TO_BE_FIXED
      accessMode: TO_BE_FIXED
      size: TO_BE_FIXED
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: thanos
  name: thanos
spec:
  helmVersion: v3
  chart:
    type: helmrepo
    repository: https://openinfradev.github.io/helm-repo
    name: thanos
    version: 3.4.0
  releaseName: thanos
  targetNamespace: lma
  values:
    global:
      storageClass: local-path
    clusterDomain: TO_BE_FIXED
    existingObjstoreSecret: TO_BE_FIXED
    query:
      nodeSelector: {}
      dnsDiscovery:
        enabled: false
        sidecarsService: TO_BE_FIXED
        sidecarsNamespace: lma
    queryFrontend:
      enabled: false
      nodeSelector: {}
      service:
        type: TO_BE_FIXED
        http:
          port: 9090
          nodePort: TO_BE_FIXED
      config: |-
        type: IN-MEMORY
        config:
          max_size: TO_BE_FIXED
          max_size_items: TO_BE_FIXED
          validity: TO_BE_FIXED
      extraFlags: []
        # query-range.split-interval: TO_BE_FIXED
        # query-range.max-retries-per-request: TO_BE_FIXED
        # query-frontend.log-queries-longer-than: TO_BE_FIXED
    bucketweb:
      enabled: TO_BE_FIXED
      logLevel: info
      refresh: 30m
      timeout: 5m
      nodeSelector: {}
      service:
        type: ClusterIP
        http:
          port: 8080
    compactor:
      enabled: TO_BE_FIXED
      logLevel: info
      retentionResolutionRaw: TO_BE_FIXED
      retentionResolution5m: TO_BE_FIXED
      retentionResolution1h: TO_BE_FIXED
      consistencyDelay: TO_BE_FIXED
      resources:
        limits:
          memory: TO_BE_FIXED
          cpu: TO_BE_FIXED
      nodeSelector: {}
      service:
        type: ClusterIP
        http:
          port: 9090
      persistence:
        enabled: true
        accessModes:
          - ReadWriteOnce
        size: TO_BE_FIXED
      extraFlags: []
        # Compaction 수행 관련 커맨트
        # 아래와 같은 block stream들이 있을때
        # external_labels: {cluster="eu1", replica="1", receive="true", environment="production"}
        # external_labels: {cluster="eu1", replica="2", receive="true", environment="production"}
        # external_labels: {cluster="us1", replica="1", receive="true", environment="production"}
        # external_labels: {cluster="us1", replica="1", receive="true", environment="staging"}
        # --compact.enable-vertical-compaction 와 함께 --deduplication.replica-label="replica" 라고 설정하면 아래와 같이 compcation됨
        # external_labels: {cluster="eu1", receive="true", environment="production"} (2 streams, resulted in one)
        # external_labels: {cluster="us1", receive="true", environment="production"}
        # external_labels: {cluster="us1", receive="true", environment="staging"}
    storegateway:
      enabled: true
      logLevel: info
      nodeSelector: {}
      service:
        type: ClusterIP
        http:
          port: 9090
        grpc:
          port: 10901
      config: |-
        type: IN-MEMORY
        config:
          max_size: TO_BE_FIXED
          max_item_size: TO_BE_FIXED
      persistence:
        enabled: true
        accessModes:
          - ReadWriteOnce
        size: TO_BE_FIXED
      replicaCount: 1
      autoscaling:
        enabled: false
        #  minReplicas: 1
        #  maxReplicas: 11
        #  targetCPU: 50
        #  targetMemory: 50
    ruler:
      enabled: TO_BE_FIXED
      logLevel: info
      alertmanagers: [] # TO_BE_FIXED
      evalInterval: 1m
      config: TO_BE_FIXED
      replicaCount: 1
      updateStrategyType: RollingUpdate
      podManagementPolicy: OrderedReady
      nodeSelector: {}
      service:
        type: ClusterIP
        http:
          port: 9090
        grpc:
          port: 10901
      persistence:
        enabled: true
        accessModes:
          - ReadWriteOnce
        size: TO_BE_FIXED
    metrics:
      enabled: false
      serviceMonitor:
        enabled: false
    volumePermissions:
      enabled: false
    minio:
      enabled: false
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: thanos-config
  name: thanos-config
spec:
  helmVersion: v3
  chart:
    type: helmrepo
    repository: https://openinfradev.github.io/helm-repo
    name: thanos-config
    version: 0.1.4
  releaseName: thanos-config
  targetNamespace: lma
  values:
    objectStorage:
      enabled: true
      secretName: TO_BE_FIXED
      bucketName: TO_BE_FIXED
      endpoint: minio:9000
      access_key: TO_BE_FIXED
      secret_key: TO_BE_FIXED
    sidecarsService:
      enabled: false
      type: NodePort
      nodePort: 30007
      name: TO_BE_FIXED
      port: 30901
      endpoints: [] # TO_BE_FIXED
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: prepare-etcd-secret
  name: prepare-etcd-secret
spec:
  helmVersion: v3
  chart:
    type: helmrepo
    repository: https://openinfradev.github.io/helm-repo
    name: prepare-etcd-secret
    version: 0.2.0
  releaseName: prepare-etcd-secret
  targetNamespace: lma
  values:
    etcd:
      certdir: /etc/kubernetes/pki/etcd
      certfile: ca.crt
      client_certfile: peer.crt
      client_keyfile: peer.key
    # master node의 label을 환경에 맞게 적어야함
    # ex) "node-role.kubernetes.io/master": ""
    nodeSelector: {} # TO_BE_FIXED
    # master node에 taint가 있는 경우 필요
    # ex) - key: "node-role.kubernetes.io/master"
    #       effect: "NoSchedule"
    #       operator: "Exists"
    tolerations: [] # TO_BE_FIXED
    deployer: "tks"
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: loki
  name: loki
spec:
  helmVersion: v3
  chart:
    type: helmrepo
    repository: https://grafana.github.io/helm-charts
    name: loki-distributed
    version: 0.58.0
  releaseName: loki
  targetNamespace: lma
  values:
    global:
      clusterDomain: cluster.local # TO_BE_FIXED
      dnsService: coredns
    loki:
      schemaConfig:
        configs:
        - from: "2020-09-07"
          store: boltdb-shipper
          object_store: s3
          schema: v11
          index:
            prefix: loki_index_
            period: 24h
      storageConfig:
        boltdb_shipper:
          active_index_directory: /var/loki/index
          cache_location: /var/loki/cache
          cache_ttl: 24h         # Can be increased for faster performance over longer query periods, uses more disk space
          shared_store: s3
        aws:
          s3: TO_BE_FIXED
          bucketnames: loki
          s3forcepathstyle: true
      structuredConfig:
        limits_config:
          ingestion_rate_mb: 25
          ingestion_burst_size_mb: 50
          max_streams_per_user: 0
          max_global_streams_per_user: 0
        table_manager:
          retention_deletes_enabled: true
          retention_period: TO_BE_FIXED
    serviceMonitor.enabled: true
    prometheusRule.enabled: true
    ingester:
      resources:
        limits:
          cpu: '4'
          memory: 4Gi
        requests:
          cpu: 100m
          memory: 250Mi
      persistence:
        enabled: true
        inMemory: false
        size: 100Gi
    memcachedExporter.enabled: true
    gateway:
      nginxConfig:
        httpSnippet: |-
          client_max_body_size 50M;
        serverSnippet: |-
          client_max_body_size 50M;
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: promtail
  name: promtail
spec:
  helmVersion: v3
  chart:
    type: helmrepo
    repository: https://grafana.github.io/helm-charts
    name: promtail
    version: 3.9.1
  releaseName: promtail
  targetNamespace: lma
  values:
    image:
      tag: 2.4.1
    extraArgs:
      - "-config.expand-env=true"
    config:
      lokiAddress: http://loki.lma:3100/loki/api/v1/push
      snippets:
        pipelineStages:
        - cri: {}
        - match:
            selector: '{app="eventrouter"}'
            stages:
            - json:
                expressions:
                  namespace: event.metadata.namespace
            - labels:
                namespace: ""
        extraScrapeConfigs: |
          - job_name: systemlog
            static_configs:
            - targets:
                - localhost
              labels:
                job: systemlog
                host: ${HOSTNAME}
                __path__: /var/log/messages
    serviceMonitor:
      enabled: false
    defaultVolumes:
      - name: containers
        hostPath:
          path: /var/lib/docker/containers
      - name: varlog
        hostPath:
          path: /var/log
    defaultVolumeMounts:
      - name: containers
        mountPath: /var/lib/docker/containers
        readOnly: true
      - name: varlog
        mountPath: /var/log
        readOnly: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: eventrouter
  name: eventrouter
spec:
  helmVersion: v3
  chart:
    type: helmrepo
    repository: https://helm-charts.wikimedia.org/stable/
    name: eventrouter
    version: 0.4.0
  releaseName: eventrouter
  targetNamespace: lma
  values:
    image:
      # Use image built from openshift eventrouter src
      repository: sktdev/os-eventrouter
      tag: 69a58b

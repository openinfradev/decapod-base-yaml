apiVersion: openinfradev.github.com/v1
kind: HelmValuesTransformer
metadata:
  name: site

global:
  # Specify nodes to install workload
  nodeSelector:
    taco-lma: enabled
  # Specify cluster name. It is useful in multi-cluster env.
  clusterName: cluster.local
  # Storageclass to install persistant
  storageClassName: taco-storage
  # Intervals to scrap the metrics in prometheuses
  serviceScrapeInterval: 10s
  federateScrapeInterval: 10s
  # Default user/password to use the object storage
  defaultPassword: password
  defaultUser: taco
  # Secret name to connect the object storage
  thanosObjstoreSecret: taco-objstore-secret
  # Which it install every thanos component?
  thanosPrimaryCluster: true

charts:
- name: prometheus-operator
  override:
    prometheusOperator.nodeSelector: $(nodeSelector)

- name: prometheus
  override:
    kubeEtcd.endpoints: [] # If your etcd is not deployed as a pod, specify IPs it can be found on
    prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.storageClassName: $(storageClassName)
    # Define specifications for storing metrics
    prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage: 200Gi
    prometheus.prometheusSpec.retention: 10d

    prometheus.prometheusSpec.externalLabels.taco_cluster: $(clusterName)
    prometheus.prometheusSpec.nodeSelector: $(nodeSelector)
    prometheus.prometheusSpec.thanos.objectStorageConfig.name: $(thanosObjstoreSecret)
    coreDns.serviceMonitor.interval: $(serviceScrapeInterval)
    kubeApiServer.serviceMonitor.interval: $(serviceScrapeInterval)
    kubeControllerManager.serviceMonitor.interval: $(serviceScrapeInterval)
    kubeDns.serviceMonitor.interval: $(serviceScrapeInterval)
    kubeEtcd.serviceMonitor.interval: $(serviceScrapeInterval)
    kubeProxy.serviceMonitor.interval: $(serviceScrapeInterval)
    kubeScheduler.serviceMonitor.interval: $(serviceScrapeInterval)

    alertmanager.service.type: NodePort
    alertmanager.service.nodePort: 30111
    alertmanager.alertmanagerSpec.alertmanagerConfigSelector.matchLabels.alertmanagerConfig: example
    alertmanager.alertmanagerSpec.nodeSelector: $(nodeSelector)
    # Define the slack target to send alerts
    alertmanager.config.global.slack_api_url: https://hooks.slack.com/services/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

    # Define for storing alert
    alertmanager.alertmanagerSpec.retention: 120h

- name: kube-state-metrics
  override:
    nodeSelector: $(nodeSelector)
    collectors.verticalpodautoscalers: false

- name: prometheus-pushgateway
  override:
    nodeSelector: $(nodeSelector)

- name: prometheus-node-exporter

- name: prometheus-process-exporter

- name: eck-resource
  override:
    kibana.nodeSelector: $(nodeSelector)
    kibana.limitCpu: 4
    kibana.limitMem: 8Gi

    elasticsearch.adminPassword: tacoword

    # ElasticSearch Deployment Architecture
    # Master nodes
    elasticsearch.nodeSets.master.nodeSelector: $(nodeSelector)
    elasticsearch.nodeSets.master.javaOpts: "-Xms2g -Xmx2g"
    elasticsearch.nodeSets.master.limitCpu: 2
    elasticsearch.nodeSets.master.limitMem: 4Gi
    elasticsearch.nodeSets.master.pvc.storageClassName: $(storageClassName)
    elasticsearch.nodeSets.master.pvc.size: 2Gi

    # Hot nodes
    elasticsearch.nodeSets.hotdata.nodeSelector: $(nodeSelector)
    elasticsearch.nodeSets.hotdata.javaOpts: "-Xms2g -Xmx2g"
    elasticsearch.nodeSets.hotdata.limitCpu: 2
    elasticsearch.nodeSets.hotdata.limitMem: 4Gi
    elasticsearch.nodeSets.hotdata.pvc.storageClassName: $(storageClassName)
    elasticsearch.nodeSets.hotdata.pvc.size: 100Gi

    # Warm nodes
    elasticsearch.nodeSets.warmdata.enabled: false
    elasticsearch.nodeSets.warmdata.nodeSelector: $(nodeSelector)
    elasticsearch.nodeSets.warmdata.javaOpts: "-Xms2g -Xmx2g"
    elasticsearch.nodeSets.warmdata.limitCpu: 1
    elasticsearch.nodeSets.warmdata.limitMem: 2Gi
    elasticsearch.nodeSets.warmdata.pvc.storageClassName: $(storageClassName)
    elasticsearch.nodeSets.warmdata.pvc.size: 200Gi

    # Client nodes
    elasticsearch.nodeSets.client.enabled: true
    elasticsearch.nodeSets.client.count: 1
    elasticsearch.nodeSets.client.nodeSelector: $(nodeSelector)
    elasticsearch.nodeSets.client.javaOpts: "-Xms2g -Xmx2g"
    elasticsearch.nodeSets.client.limitCpu: 2
    elasticsearch.nodeSets.client.limitMem: 4Gi
    elasticsearch.nodeSets.client.pvc.storageClassName: $(storageClassName)

- name: grafana
  override:
    adminPassword: password
    persistence.storageClassName: $(storageClassName)

- name: fluentbit-operator
  override:
    global.base_cluster_url: $(clusterName)
    fluentbitOperator.nodeSelector: $(nodeSelector)
    logExporter.nodeSelector: $(nodeSelector)

- name: fluentbit
  override:
    global.base_cluster_url: $(clusterName)
    global.nodeSelector: $(nodeSelector)
    fluentbit.clusterName: $(clusterName)

    # Define a endpoint to store logs
    fluentbit.outputs.es.host: eck-elasticsearch-es-http.lma.svc.$(clusterName)
    fluentbit.outputs.es.elasticPasswordSecret: eck-elasticsearch-es-elastic-user
    fluentbit.outputs.kafka:
      enabled: false
      broker: "YOUR_BORKER_INFO"
      topic: "YOUR_TACO_LOG_INFO"
    fluentbit.nodeSelector: $(nodeSelector)

- name: addons
  override:
    # Configure the connection from prometheus to elasticsearch (It not used)
    metricbeat.elasticsearch.host: https://eck-elasticsearch-es-http.lma.svc.$(clusterName):9200
    metricbeat.kibana.host: eck-kibana-dashboard-kb-http.lma.svc.$(clusterName):5601
    metricbeat.prometheus.hosts:
    - thanos-query-frontend.lma.svc.$(clusterName):9090

    # Initialize the kibana index
    kibanaInit.url: http://eck-kibana-dashboard-kb-http.lma.svc.$(clusterName):5601
    serviceMonitor.grafana.interval: $(serviceScrapeInterval)
    serviceMonitor.argocd.interval: $(serviceScrapeInterval)
    serviceMonitor.argowf.interval: $(serviceScrapeInterval)
    serviceMonitor.processExporter.interval: $(serviceScrapeInterval)
    serviceMonitor.ceph.interval: $(serviceScrapeInterval)
    serviceMonitor.calico.interval: $(serviceScrapeInterval)
    serviceMonitor.kubeStateMetrics.interval: $(serviceScrapeInterval)
    serviceMonitor.nodeExporter.interval: $(serviceScrapeInterval)
    serviceMonitor.kubelet.interval: $(serviceScrapeInterval)
    serviceMonitor.istio.interval: $(serviceScrapeInterval)
    serviceMonitor.jaeger.interval: $(serviceScrapeInterval)

- name: prometheus-adapter
  override:
    nodeSelector: $(nodeSelector)

- name: kubernetes-event-exporter
  override:
    conf.default.hosts:
    - TO_BE_FIXED

- name: thanos
  override:
    global.storageClass: $(storageClassName)
    clusterDomain: $(clusterName)
    existingObjstoreSecret: $(thanosObjstoreSecret)
    query.nodeSelector: $(nodeSelector)
    query.dnsDiscovery.sidecarsService: lma-thanos-discovery
    # query.stores:
    #   - lma-thanos-discovery.lma.svc.$(clusterName)
    queryFrontend.nodeSelector: $(nodeSelector)
    queryFrontend.service.type: NodePort
    queryFrontend.service.http.nodePort: 30007
    bucketweb.nodeSelector: $(nodeSelector)
    compactor.nodeSelector: $(nodeSelector)
    storegateway.nodeSelector: $(nodeSelector)
    ruler.nodeSelector: $(nodeSelector)

    queryFrontend.config: |-
        type: IN-MEMORY
        config:
          max_size: 512MB
          max_size_items: 100
          validity: 100s

    queryFrontend.extraFlags: 
      - --query-range.split-interval=24h
      - --query-range.max-retries-per-request=3
      - --query-frontend.log-queries-longer-than=60s

    bucketweb.enabled: $(thanosPrimaryCluster)
    compactor.enabled: $(thanosPrimaryCluster)
    compactor.retentionResolutionRaw: 30d
    compactor.retentionResolution5m: 30d
    compactor.retentionResolution1h: 10y
    compactor.consistencyDelay: 30m
    compactor.resources.limits.memory: 1024Mi
    compactor.resources.limits.cpu: 1000m
    compactor.persistence.size: 8Gi
    compactor.extraFlags: 
    storegateway.config: |-
        type: IN-MEMORY
        config:
          max_size: 250MB
          max_item_size: 125MB
    storegateway.persistence.size: 8Gi
    ruler.enabled: $(thanosPrimaryCluster)
    ruler.alertmanagers:
    - http://alertmanager-operated:9093
    ruler.config: |-
        groups:
          - name: "metamonitoring"
            rules:
              - alert: "PrometheusDown"
                expr: absent(up{prometheus="monitoring/prometheus-operator"})
    ruler.persistence.size: 8Gi
    minio.accessKey.password: $(defaultUser)
    minio.secretKey.password: $(defaultPassword)
    minio.defaultBuckets: thanos
    minio.persistence.storageClass: $(storageClassName)
    minio.persistence.accessMode: ReadWriteOnce
    minio.persistence.size: 8Gi

- name: thanos-config
  override:
    objectStorage:
      bucketName: thanos
      endpoint: thanos-minio.lma.svc.$(clusterName):9000
      access_key: $(defaultUser)
      secret_key: $(defaultPassword)
      secretName: $(thanosObjstoreSecret)
    sidecarsService.name: thanos-sidecars
    sidecarsService.endpoints: 
      - 172.16.50.114

- name: loki
  override:
    replicas: 1
    persistence.size: 20Gi
    persistence.storageClassName: $(storageClassName)
    serviceMonitor.enabled: false

- name: promtail
  override:
    config.lokiAddress: http://loki.lma:3100/loki/api/v1/push
    serviceMonitor.enabled: false
    syslogService.enabled: false
